---
title: "Chapter4.Rmd"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r cars}
summary(cars)
```

```{r pressure, echo=FALSE}
plot(pressure)
```

#Chapter 4 - Clustering and classification

First we load the "Boston" data from MASS package and explore it.
```{r}
library(MASS)
data("Boston")
str(Boston)
summary(Boston)
names(Boston)
```
The "Boston"-data is comprised of 506 observations of 14 different variables. The "Names()"-function provides the names of the variables included. The function "Summary()" produces the distributions of each variable. 

Now we explore the relationship between variables.
```{r}
library(corrplot)
library(tidyverse)
cor_matrix<-cor(Boston)  %>% round(digits=2)
corrplot(cor_matrix, type="upper", cl.pos = "b",tl.pos = "d",tl.cex = 0.6  )
```
In the graph, the blue colour represents positive correlation between variables while the red rpresents the negatives. The darker the colour and the bigger the ball, the stronger the correlation is. For example, the big blue ball between "rad" and "tax" tells that there is a strong correlation between access to highways and property tax rate.

Now we'll standardise the data
```{r}
boston_scaled <- scale(Boston)
summary(boston_scaled)
```
Now all the observations are using the same scale. Making assumptions about the summary for example is meaningful. 

Then we must turn "boston_scaled" back into a data.frame
```{r}
boston_scaled <- as.data.frame(boston_scaled)
```

These lines give us the cathegorical variable "crime"
```{r}
bins <- quantile(boston_scaled$crim)
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
```

And these replace the old variable "crim" with the cathegorical crime
```{r}
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
```

Then we divide the data to train with 80% of the data and test with 20% of the data.
```{r}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
```


5. Because the data has now been cathegorised into separate sets for training and testing, we can use the training-part for linear discriminant analysis, where crime rate will be predicted by all of the other variables.  

```{r ex_5, fig.height=8, fig.width=12}
lda.fit <- lda(crime ~ ., data = train)
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

classes <- as.numeric(train$crime)

plot(lda.fit, dimen = 2, col=classes)
lda.arrows(lda.fit, myscale = 2)

```
Looking at the bi-plot, it's clear that the "rad"-variable acts (on its own) as a predictor of "high crime rate" in the Boston data. The other 12 variables are associated with low, medium low and medium high rates of crime. The grouping based on these 12 variables is quite vague and it is difficult to see whether any of the variables can accurately/adequately sort the associated observations.

6.
```{r}
crime_cat<-test$crime
test<-dplyr::select(test, -crime)
lda.pred<-predict(lda.fit, newdata = test)
table(correct = crime_cat, predicted = lda.pred$class)
```
The amount of correct cases and of predicted cases for each of the cathegories (low, med_low, med_high, high) varies between every sample matrix. The change is to be expected as the sets we have created have been randomly classified. There is much less variation between predictions for the "high" class than there is in the predictions of the others.


7. 
Before we practice K-means clustering, we should reload the Boston data, scale it, and afterwards measure the distances between the observations. 

```{r}
data(Boston)
boston_scaled1<-as.data.frame(scale(Boston))
dist_eu<-dist(boston_scaled1)
summary(dist_eu)
head(boston_scaled1)
```
The scaled Boston data will now be used for K-means clustering. It isn't trivial (in many cases) to investigate on the number of clusters that can classify the data. Therefore, we need to first randomize the usage of a certain number of clusters.

First let's start with a random number cluster. Let us choose k=4 and apply k-means on the data.
```{r}
kmm = kmeans(boston_scaled1,6,nstart = 50 ,iter.max = 15) 
```

The elbow method is one good technique using which we can estimate the number of clusters.
```{r}
library(ggplot2)
set.seed(1234)
k.max <- 15
data <- boston_scaled1
wss <- sapply(1:k.max, 
              function(k){kmeans(data, k)$tot.withinss})
qplot(1:k.max, wss, geom = c("point", "line"), span = 0.2,
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
```
  
The elbow plot seems to indicate that we may not find more than two clear clusters but it's good to confirm predictions using another method because there is no shortage of methods for analyses like these. Let's try the "NbClust"- package.
```{r}
library(NbClust)
nb <- NbClust(boston_scaled1, diss=NULL, distance = "euclidean", 
              min.nc=2, max.nc=5, method = "kmeans", 
              index = "all", alphaBeale = 0.1)
hist(nb$Best.nc[1,], breaks = max(na.omit(nb$Best.nc[1,])))
```
Now, it's easier to see that the data is described better with two clusters. With that, we should run the k-means algorithm again. 


```{r, fig.height=10, fig.width=10}
km_final = kmeans(boston_scaled1, centers = 2) 
pairs(boston_scaled1[3:9], col=km_final$cluster)
```


The clusters in the above plot are divided into two groups and outlined using the colors red and black. Some of the pairs are better grouped than other ones in the plot. One of the important observations can be made with the "chas"-variable where the observations in all of the pairs formed by it are wrongly clustered. Still, clusters formed by the "rad" variable are better separated.


Bonus:

Now, we will use a randomly selected cluster number (k=6) and perform LDA. We shall follow the the basic steps of scaling and distance calculation. Afterwards, we will find out how the biplot looks (of the whole data set) as we try to group it into six different categories.

```{r, fig.width=9, fig.height=9}
boston_scaled2<-as.data.frame(scale(Boston))
head(boston_scaled2)
set.seed(1234)
km_bs2<-kmeans(dist_eu, centers = 6)
head(km_bs2)
myclust<-data.frame(km_bs2$cluster)
boston_scaled2$clust<-km_bs2$cluster
head(boston_scaled2)
lda.fit_bs2<-lda(clust~., data = boston_scaled2 )
lda.fit_bs2
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
plot(lda.fit_bs2, dimen = 2)
lda.arrows(lda.fit_bs2, myscale = 3)
```
  
I think the dataset could be grouped more easily by a lower number of clusters, though I'm not sure what would be the minimum number. The top three most relevant variables according to our bi-plot are "zn", "nox", and "tax".  


Super.Bonus

Additional ways for visualising LDA:
```{r, fig.width=8, fig.height=8}
library(plotly)
model_predictors <- dplyr::select(train, -crime)
dim(model_predictors)
dim(lda.fit$scaling)
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = train$crim)
```

```{r, fig.width=8, fig.height=8}
k_means_matpro <- kmeans(matrix_product, centers = 6, iter.max = 10, nstart = 1, trace=FALSE)
head(train)
myclust <- NA
train$cl<-myclust
boston_scaled2$cl<-myclust
head(boston_scaled2)
head(train)
rownames(train)
rownames(boston_scaled2)
train$cl <- boston_scaled2$clust[match(rownames(train), rownames(boston_scaled2))]
head(train)
nrow(train)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type = "scatter3d", mode="markers", color = train$cl)
```
In light of my research, clustering made with K-means have turned out to be more informative than thes one based on crime classes. 

